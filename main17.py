# -*- coding: utf-8 -*-
"""
@File    : main17.py
@Author  : qy
@Date    : 2025/8/26
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, AsyncGenerator, Optional
import httpx
import logging
import json
import uuid
import asyncio
import time
import os
from datetime import datetime, timedelta

from starlette.responses import StreamingResponse
from company_info import CompanyInfo
from policy_utils import get_policy_info
from prompt import (
    build_policy_elements_prompt,
    build_company_judgment_prompt,
    empty_company_info_dict,
    build_company_standardization_prompt
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

API_URL = "https://mcc-pre.3xmt.com/gateway/ai-service/v1/chat/completions"
API_KEY = "sk-bFcPcwS7J7oP6e8LGo"

app = FastAPI()

# ä¼šè¯å†å²å­˜å‚¨æ–‡ä»¶è·¯å¾„
SESSION_HISTORY_FILE = "session_history.json"
# ä¼šè¯è¿‡æœŸæ—¶é—´ï¼ˆ6å°æ—¶ï¼‰
EXPIRY_HOURS = 6
# æ¯ä¸ª session_id çš„æœ€å¤§å†å²è®°å½•æ•°
MAX_HISTORY_PER_SESSION = 10


def load_session_history() -> Dict[str, List[Dict[str, Any]]]:
    """ä»æ–‡ä»¶åŠ è½½ä¼šè¯å†å²"""
    if os.path.exists(SESSION_HISTORY_FILE):
        try:
            with open(SESSION_HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            logger.error("ä¼šè¯å†å²æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸")
            return {}
    return {}


def save_session_history(history: Dict[str, List[Dict[str, Any]]]):
    """ä¿å­˜ä¼šè¯å†å²åˆ°æ–‡ä»¶"""
    try:
        with open(SESSION_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜ä¼šè¯å†å²å¤±è´¥: {str(e)}")


def clean_expired_sessions(history: Dict[str, List[Dict[str, Any]]], current_time: float,
                           session_id: Optional[str] = None) -> Dict[str, List[Dict[str, Any]]]:
    """æ¸…ç†è¿‡æœŸä¼šè¯"""
    cleaned_history = {}
    expiry_threshold = current_time - (EXPIRY_HOURS * 3600)
    for sid, messages in history.items():
        if not messages:
            continue
        # æ£€æŸ¥æœ€åä¸€æ¡è®°å½•çš„æ—¶é—´æˆ³
        last_timestamp = messages[-1].get('timestamp', 0) if messages[-1].get('timestamp') else current_time
        if sid == session_id or last_timestamp >= expiry_threshold:
            cleaned_history[sid] = messages
        else:
            logger.info(
                f"æ¸…ç†è¿‡æœŸä¼šè¯: session_id={sid}, last_timestamp={datetime.fromtimestamp(last_timestamp).strftime('%Y-%m-%d %H:%M:%S')}")
    return cleaned_history


SESSION_HISTORY: Dict[str, List[Dict[str, Any]]] = load_session_history()


def merge_company_info(old: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:
    """å°† new ä¸­éç©ºå­—æ®µåˆå¹¶åˆ° old"""
    merged = old.copy()
    for k, v in new.items():
        if v is not None and (v != "" and (not isinstance(v, list) or len(v) > 0)):
            merged[k] = v
    return merged


def filter_non_empty_fields(d: dict) -> dict:
    result = {}
    for k, v in d.items():
        if v is None:
            continue
        if isinstance(v, str) and not v.strip():
            continue
        if isinstance(v, list) and len(v) == 0:
            continue
        result[k] = v
    return result


class NewCheckRequest(BaseModel):
    part_id: str
    metadata: CompanyInfo
    user_input_text: Optional[str] = None
    session_id: Optional[str] = None
    messages: Optional[List[Dict[str, Any]]] = None


async def stream_model_response(prompt: str, buffer_size: int = 2, flush_interval: float = 0.06) -> AsyncGenerator[
    str, None]:
    payload = {
        "model": "deepseek-v3",
        "messages": [{"role": "user", "content": prompt}],
        "stream": True
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    buffer = ""
    finished = False

    async def fetch_model():
        nonlocal buffer, finished
        try:
            async with httpx.AsyncClient(timeout=None) as client:
                async with client.stream("POST", API_URL, json=payload, headers=headers) as response:
                    async for line_bytes in response.aiter_lines():
                        if not line_bytes or not line_bytes.startswith("data:"):
                            continue
                        line_str = line_bytes[len("data:"):].strip()
                        if not line_str:
                            continue
                        try:
                            data = json.loads(line_str)
                            delta = data["choices"][0]["delta"].get("content", "")
                            if delta:
                                buffer += delta
                            finish_reason = data["choices"][0].get("finish_reason")
                            if finish_reason == "stop":
                                finished = True
                                break
                        except json.JSONDecodeError:
                            continue
        except httpx.RequestError:
            buffer += "[æ¨¡å‹æœåŠ¡è¯·æ±‚å¤±è´¥]"
            finished = True

    fetch_task = asyncio.create_task(fetch_model())

    try:
        while not finished or buffer:
            if buffer:
                out, buffer = buffer[:buffer_size], buffer[buffer_size:]
                yield out
            await asyncio.sleep(flush_interval)
        yield "\n"
    finally:
        fetch_task.cancel()
        await asyncio.sleep(0)


async def collect_model_output(prompt: str) -> str:
    result = ""
    async for chunk in stream_model_response(prompt, buffer_size=64, flush_interval=0.02):
        result += chunk
    return result.strip()


async def chunked_response(generator: AsyncGenerator[Dict[str, Any], None]):
    async def iterate():
        async for text_dict in generator:
            text_bytes = json.dumps(text_dict, ensure_ascii=False).encode("utf-8")
            output = b"data:" + text_bytes + b"\n\n"
            yield output

    return StreamingResponse(iterate(), media_type="text/event-stream")


@app.post("/check_policy")
async def check_policy(req: NewCheckRequest):
    # è·å–å½“å‰æ—¶é—´æˆ³
    current_time = time.time()

    # æ¸…ç†è¿‡æœŸä¼šè¯ï¼Œä¿ç•™å½“å‰ session_id çš„è®°å½•
    global SESSION_HISTORY
    SESSION_HISTORY = clean_expired_sessions(SESSION_HISTORY, current_time, req.session_id or str(uuid.uuid4()))
    save_session_history(SESSION_HISTORY)

    # ä½¿ç”¨ session_idï¼Œå¦‚æœæ²¡æœ‰å°±æ–°å»º
    session_id = req.session_id or str(uuid.uuid4())
    if session_id not in SESSION_HISTORY:
        SESSION_HISTORY[session_id] = []

    # åˆå§‹åŒ–åŸºç¡€å…¬å¸ä¿¡æ¯
    base_company_info = req.metadata.dict()

    # è·å–æœ€æ–°çš„å…¬å¸ä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨ä¼šè¯å†å²ä¸­çš„æœ€æ–°ä¿¡æ¯ï¼Œæ²¡æœ‰åˆ™ä½¿ç”¨åŸºç¡€ä¿¡æ¯
    if SESSION_HISTORY[session_id]:
        last_message = SESSION_HISTORY[session_id][-1]['content']
        if isinstance(last_message, str):
            try:
                last_company_info = json.loads(last_message)
            except json.JSONDecodeError:
                last_company_info = base_company_info
        else:
            last_company_info = last_message
    else:
        last_company_info = base_company_info


    # è·å–æ”¿ç­–ä¿¡æ¯
    policy_info = get_policy_info(req.part_id)
    if "error" in policy_info:
        raise HTTPException(status_code=404, detail='æœªæ‰¾åˆ°è¯¥ç”³æŠ¥ä¸“é¡¹æ”¿ç­–')

    # æ„å»ºæ”¿ç­–è¦ç´ æç¤ºè¯ï¼ˆå¯ä»¥ç«‹å³æ‰§è¡Œï¼Œä¸ä¾èµ–æ ‡å‡†åŒ–ï¼‰
    policy_elements_prompt = build_policy_elements_prompt(req.part_id, policy_info)

    first_intro = (
        "æ¬¢è¿è¿›å…¥æ™ºèƒ½å¸®åŠç¯èŠ‚ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ™ºèƒ½åŒ¹é…ã€æ™ºèƒ½ä½“æ£€ã€æ™ºèƒ½å¡«æŠ¥ç­‰ç¯èŠ‚çš„ç”³æŠ¥å…¨æµç¨‹æ™ºèƒ½è¾…åŠ©æœåŠ¡ï¼Œ"
        "æ‚¨å¯ä»¥ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ä¸ºæ‚¨æ¨èçš„ç›¸å…³ç”³æŠ¥æ”¿ç­–ã€‚æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥å‘Šè¯‰æˆ‘æƒ³è¦ç”³æŠ¥çš„æ”¿ç­–ï¼Œç”±æˆ‘æä¾›æ™ºèƒ½ä½“æ£€æœåŠ¡ã€‚\n"
        "ä¸‹é¢å±•ç¤ºç”³è¯·ä¸“é¡¹æ”¿ç­–è¦ç´ ï¼š\n"
    )
    second_intro = (
        "ç»AIå¯¹æ”¿ç­–ç”³æŠ¥è¦æ±‚ä¸è´µå¸ç”»åƒç‰¹å¾æ™ºèƒ½åˆ†æï¼Œæ‚¨å½“å‰è¿˜ä¸æ»¡è¶³æ”¿ç­–ç”³æŠ¥æ¡ä»¶ï¼Œ"
        "è¿˜å­˜åœ¨ä»¥ä¸‹æ¡ä»¶éœ€è¦ç”±æ‚¨ç¡®è®¤ï¼š\n"
    )

    created_ts = int(current_time)
    choice_index = 0

    async def event_stream() -> AsyncGenerator[Dict[str, Any], None]:
        nonlocal choice_index

        queue1: asyncio.Queue[str] = asyncio.Queue()
        queue2: asyncio.Queue[str] = asyncio.Queue()

        async def model_task(prompt: str, queue: asyncio.Queue):
            async for chunk in stream_model_response(prompt):
                await queue.put(chunk)
            await queue.put(None)

        # ğŸ”¹ æ ‡å‡†åŒ–ä»»åŠ¡
        async def standardize_user_input(user_input_text: str, last_company_info: Dict[str, Any]):
            standardization_prompt = build_company_standardization_prompt(user_input_text)
            standardized_info_str = await collect_model_output(standardization_prompt)
            standardized_info_str = standardized_info_str.strip("```json").strip("```").strip()
            try:
                standardized_info_dict = json.loads(standardized_info_str)
            except json.JSONDecodeError:
                logger.warning(f"æ— æ³•è§£ææ ‡å‡†åŒ–ä¿¡æ¯: {standardized_info_str}")
                standardized_info_dict = {}
            merged_info = merge_company_info(last_company_info, standardized_info_dict)
            merged_info = merge_company_info(empty_company_info_dict(), merged_info)
            return merged_info

        if req.user_input_text and req.user_input_text.strip():
            standardize_task = asyncio.create_task(standardize_user_input(req.user_input_text, last_company_info))
        else:
            async def return_last_info():
                merged_info = merge_company_info(empty_company_info_dict(), last_company_info)
                return merged_info
            standardize_task = asyncio.create_task(return_last_info())

        # ğŸ”¹ æ¨¡å‹ä»»åŠ¡1ï¼šæ”¿ç­–è¦ç´ ï¼ˆç«‹å³è·‘ï¼‰
        task1 = asyncio.create_task(model_task(policy_elements_prompt, queue1))

        # ğŸ”¹ æ¨¡å‹ä»»åŠ¡2ï¼šä¼ä¸šåˆ¤æ–­ï¼ˆç­‰å¾…æ ‡å‡†åŒ–åå†è·‘ï¼‰
        async def wait_and_run_judgment():
            merged_info = await standardize_task

            logger.info(f"[è°ƒè¯•] æ›´æ–°åè¦é€ç»™å¤§æ¨¡å‹çš„å…¬å¸ä¿¡æ¯: {json.dumps(merged_info, ensure_ascii=False, indent=2)}")

            new_message = {
                "role": "user",
                "content": filter_non_empty_fields(merged_info),
                "timestamp": time.time()
            }
            SESSION_HISTORY[session_id].append(new_message)
            if len(SESSION_HISTORY[session_id]) > MAX_HISTORY_PER_SESSION:
                SESSION_HISTORY[session_id] = SESSION_HISTORY[session_id][-MAX_HISTORY_PER_SESSION:]
            save_session_history(SESSION_HISTORY)

            # å†æ„å»ºä¼ä¸šåˆ¤æ–­æç¤ºè¯
            company_judgment_prompt = build_company_judgment_prompt(merged_info, policy_info)
            await model_task(company_judgment_prompt, queue2)

        task2 = asyncio.create_task(wait_and_run_judgment())

        # æ‰“å­—æœºæ•ˆæœè¾“å‡º
        async def send_typing_text_json(text: str, speed: float = 0.06) -> AsyncGenerator[Dict[str, Any], None]:
            nonlocal choice_index
            buffer = text
            while buffer:
                out, buffer = buffer[:2], buffer[2:]
                chunk_data = {
                    "id": session_id,
                    "model": "deepseek-v3",
                    "created": created_ts,
                    "part_id": req.part_id,
                    "choices": [
                        {
                            "index": choice_index,
                            "finish_reason": None,
                            "message": {
                                "content": out,
                                "role": "assistant"
                            }
                        }
                    ]
                }
                choice_index += 1
                yield chunk_data
                await asyncio.sleep(speed)

        # è¾“å‡ºå¼€å¤´è¯­1
        async for chunk in send_typing_text_json(first_intro):
            yield chunk

        # æ¶ˆè´¹æ¨¡å‹1 - æ”¿ç­–è¦ç´ 
        while True:
            chunk = await queue1.get()
            if chunk is None:
                break
            yield {
                "id": session_id,
                "model": "deepseek-v3",
                "created": created_ts,
                "part_id": req.part_id,
                "choices": [
                    {
                        "index": choice_index,
                        "finish_reason": None,
                        "message": {
                            "content": chunk,
                            "role": "assistant"
                        }
                    }
                ]
            }
            choice_index += 1

        # ç©ºè¡Œåˆ†éš”
        yield {
            "id": session_id,
            "model": "deepseek-v3",
            "created": created_ts,
            "part_id": req.part_id,
            "choices": [
                {
                    "index": choice_index,
                    "finish_reason": None,
                    "message": {"content": "", "role": "assistant"}
                }
            ]
        }
        choice_index += 1

        # è¾“å‡ºå¼€å¤´è¯­2
        async for chunk in send_typing_text_json(second_intro):
            yield chunk

        # æ¶ˆè´¹æ¨¡å‹2 - ä¼ä¸šåˆ¤æ–­
        last_chunk = None
        while True:
            chunk = await queue2.get()
            if chunk is None:
                break
            last_chunk = chunk
            yield {
                "id": session_id,
                "model": "deepseek-v3",
                "created": created_ts,
                "part_id": req.part_id,
                "choices": [
                    {
                        "index": choice_index,
                        "finish_reason": None,
                        "message": {
                            "content": chunk,
                            "role": "assistant"
                        }
                    }
                ]
            }
            choice_index += 1

        if last_chunk is not None:
            yield {
                "id": session_id,
                "model": "deepseek-v3",
                "created": created_ts,
                "part_id": req.part_id,
                "choices": [
                    {
                        "index": choice_index,
                        "finish_reason": "stop",
                        "message": {
                            "content": last_chunk,
                            "role": "assistant"
                        }
                    }
                ]
            }

    return await chunked_response(event_stream())



if __name__ == '__main__':
    import uvicorn

    uvicorn.run(app="main17:app", workers=1, host="0.0.0.0", port=8001, reload=False)
